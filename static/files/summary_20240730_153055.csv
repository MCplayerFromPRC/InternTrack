dataset,version,metric,mode,official_Ampere2.5_7B_1.0.1_28000
WSC,7902a7,accuracy,gen,53.26
MultiRC,27071f,accuracy,gen,87.22
lambada,28e9a1,accuracy,gen,80.01
commonsense_qa,c946f2,accuracy,gen,86.98
triviaqa_wiki_1shot,20a989,score,gen,54.27
nq_open_1shot,20a989,score,gen,13.88
race-high,6e27c0,accuracy,ppl,65.18
winogrande,252f01,accuracy,unknown,76.56
hellaswag,59c85e,accuracy,ppl,50.4
gsm8k,17d0dc,accuracy,gen,36.24
math,db136b,accuracy,gen,13.58
math_0shot,393424,accuracy,gen,0.84
TheoremQA,6f0af8,score,gen,16.38
openai_humaneval,d2537e,humaneval_pass@1,gen,37.2
openai_humaneval_v2,ce6b06,humaneval_pass@1,gen,39.63
sanitized_mbpp,742f0c,score,gen,51.36
GPQA_diamond,6bf57a,accuracy,ppl,22.73
mmlu-stem,8Cf043,naive_average,ppl,51.54
mmlu-social-science,bc21ac,naive_average,ppl,71.2
mmlu-humanities,4c6dd2,naive_average,ppl,65.45
mmlu-other,07f386,naive_average,ppl,65.13
cmmlu-stem,476d52,naive_average,ppl,47.43
cmmlu-social-science,131a45,naive_average,ppl,67.59
cmmlu-humanities,5404f8,naive_average,ppl,66.24
cmmlu-other,58083b,naive_average,ppl,68.52
cmmlu-china-specific,959c0b,naive_average,ppl,64.34
ceval-test,a63c14,naive_average,ppl,58.87
ceval-test-stem,f76f38,naive_average,ppl,50.99
ceval-test-social-science,fb1eb5,naive_average,ppl,71.45
ceval-test-humanities,ae6016,naive_average,ppl,63.09
ceval-test-other,67a724,naive_average,ppl,57.54
ceval-test-hard,ac43cf,naive_average,ppl,39.21
ceval-test,9cec05,correct_bpb-incorrect_bpb,ppl,237.83
race-high,c94731,correct_bpb-incorrect_bpb,ppl,51.08
hellaswag,d09684,correct_bpb-incorrect_bpb,ppl,333.03
GPQA_diamond,6bf57a,correct_bpb-incorrect_bpb,ppl,54.74
