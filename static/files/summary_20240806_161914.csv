dataset,version,metric,mode,ampere_7B_FT_0.17rc8_32k_0802_3755
WiC,d06864,accuracy,gen,57.21
summedits,315438,accuracy,gen,38.19
chid-dev,211ee7,accuracy,gen,75.25
afqmc-dev,901306,accuracy,gen,70.97
bustm-dev,5cc669,accuracy,gen,73.75
cluewsc-dev,5ab83b,accuracy,gen,79.25
WSC,7902a7,accuracy,gen,57.69
winogrande,a9ede5,accuracy,gen,66.77
C3,8c358f,accuracy,gen,90.63
CMRC_dev,1bd3c8,score,gen,69.99
DRCD_dev,1bd3c8,score,gen,80.08
MultiRC,27071f,accuracy,gen,84.22
race-middle,9a54b6,accuracy,gen,89
race-high,9a54b6,accuracy,gen,83.7
openbookqa_fact,60bd99,accuracy,gen,91.8
drop,8a9ed9,score,gen,68.97
csl_dev,25149e,accuracy,gen,49.38
lcsts,8ee1fe,rouge1,gen,29.81
Xsum,31397e,rouge1,gen,38.48
eprstmt-dev,9710da,accuracy,gen,39.38
lambada,2.17E+13,accuracy,gen,79.9
tnews-dev,958876,accuracy,gen,54.83
BoolQ,883d50,accuracy,gen,88.04
commonsense_qa,c946f2,accuracy,gen,86.98
nq,c788f6,score,gen,27.78
triviaqa,2121ce,score,gen,56.33
tydiqa-goldp_english,38da40,f1,gen,36.58
ARC-c,1e0de5,accuracy,gen,86.44
ARC-e,1e0de5,accuracy,gen,93.47
wikibench-wiki-single_choice_cncircular,f96ece,perf_4,gen,34.05
commonsenseqa_cn,d380d0,accuracy,gen,71.09
nq_cn,141737,score,gen,12.27
cmnli,1abf97,accuracy,gen,68.1
ocnli,c4cb6c,accuracy,gen,63.59
ocnli_fc-dev,51e956,accuracy,gen,68.12
AX_b,4dfefa,accuracy,gen,68.75
AX_g,68aac7,accuracy,gen,83.43
RTE,68aac7,accuracy,gen,75.81
ReCoRD,30dea0,score,gen,50.89
hellaswag,6faab5,accuracy,gen,81.87
piqa,1194eb,accuracy,gen,83.79
gsm8k-extra-options,108724,perf_4,gen,74.15
openai_humaneval,6d1cc2,humaneval_pass@1,gen,50.61
mbpp,caa7ab,score,gen,43.6
py150,38b13d,score,gen,25.93
maxmin,c205fb,accuracy,gen,62.74
math-agent,0c1b4e,follow_acc,unknown,34.76
cibench_generation_SciPy,1bc17b,executable,unknown,73.8
cibench_generation_Seaborn,1bc17b,executable,unknown,53.77
cibench_generation_PyTorch,1bc17b,executable,unknown,66.74
plugin_eval-instruct_v1,10482d,format_metric,unknown,80.7
plugin_eval-instruct_v1,10482d,args_em_metric,unknown,73.65
gov_report_4k,54c5b0,score,gen,31.51
gov_report_8k,54c5b0,score,gen,30.74
hotpotqa_e_15k,6b3efc,score,gen,60.88
lines_2k,9914f5,score,gen,100
lines_4k,9914f5,score,gen,98
lines_8k,9914f5,score,gen,99
lines_15k,7daf4a,score,gen,96
lines_30k,70f645,score,gen,50
multifieldqa_zh_4k,e9a7ef,score,gen,56.46
passage_retrieval_zh_8k,01cca2,score,gen,100
stackselect_2k,03f1f8,score,gen,48.5
stackselect_4k,03f1f8,score,gen,34.5
stackselect_8k,03f1f8,score,gen,13.5
stackselect_15k,c92c67,score,gen,2.5
stackselect_30k,42fd23,score,gen,1
textsort_2k,28f0f7,score,gen,6
textsort_4k,28f0f7,score,gen,7
textsort_8k,28f0f7,score,gen,4
textsort_15k,4d7bcb,score,gen,4
textsort_30k,781c84,score,gen,4
trec_e_2k,824187,score,gen,50
trec_e_4k,824187,score,gen,63.83
trec_e_8k,824187,score,gen,75
trec_e_15k,824187,score,gen,78.05
ds1000_Pandas,353ae7,accuracy,gen,7.22
ds1000_Numpy,353ae7,accuracy,gen,4.09
ceval-middle_school_geography,8a63be,accuracy,gen,75
ceval-middle_school_politics,5be3e7,accuracy,gen,76.19
ceval-teacher_qualification,4e4ced,accuracy,gen,81.82
ceval-art_studies,2a1300,accuracy,gen,66.67
ceval-chinese_language_and_literature,0f8b68,accuracy,gen,65.22
ceval-high_school_chinese,315705,accuracy,gen,68.42
ceval-high_school_history,7eb30a,accuracy,gen,80
ceval-ideological_and_moral_cultivation,a2aa4a,accuracy,gen,94.74
ceval-law,a110a1,accuracy,gen,37.5
ceval-legal_professional,ce8787,accuracy,gen,43.48
agieval-aqua-rat,c090ca,accuracy,gen,24.02
agieval-math,e0c6d9,score,gen,15.8
agieval-logiqa-en,c6ee60,accuracy,gen,42.09
agieval-logiqa-zh,03474d,accuracy,gen,46.54
agieval-jec-qa-kd,b1e586,accuracy,gen,27.4
agieval-jec-qa-ca,47bc29,accuracy,gen,25.5
agieval-lsat-ar,ed1edf,accuracy,gen,21.74
agieval-lsat-lr,ec6882,accuracy,gen,47.25
agieval-lsat-rc,33077d,accuracy,gen,67.66
agieval-sat-math,6c970d,accuracy,gen,35.91
agieval-sat-en,4e3fef,accuracy,gen,85.92
agieval-sat-en-without-passage,4e3fef,accuracy,gen,42.72
agieval-gaokao-chinese,774562,accuracy,gen,80.08
agieval-gaokao-english,cb5bc6,accuracy,gen,87.91
agieval-gaokao-geography,2ca56f,accuracy,gen,81.41
agieval-gaokao-history,9c3ae0,accuracy,gen,87.66
agieval-gaokao-biology,277c85,accuracy,gen,78.1
lukaemon_mmlu_astronomy,4dd3a8,accuracy,gen,66.45
lukaemon_mmlu_business_ethics,dc6ec0,accuracy,gen,65
lukaemon_mmlu_clinical_knowledge,7437b5,accuracy,gen,67.17
lukaemon_mmlu_college_biology,9c8f5e,accuracy,gen,72.92
lukaemon_mmlu_college_chemistry,e44abe,accuracy,gen,48
lukaemon_mmlu_college_computer_science,37bffd,accuracy,gen,52
lukaemon_mmlu_college_mathematics,44e388,accuracy,gen,34
lukaemon_mmlu_college_medicine,f6342e,accuracy,gen,60.69
